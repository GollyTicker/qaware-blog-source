---
title: "Binary Data Format Comparison"
date: 2021-10-22T14:16:45+02:00
lastmod: 2021-10-22T14:16:45+02:00
author: "[Nicolai Ommer](https://github.com/g3force)"
type: "post"
image: "binary-data-format-comparison/title.jpg"
tags: ["Memory", "Performance", "Java", "Storage", "Comparison"]
summary: A comparison of different binary data formats in terms of read/write performance and storage usage.
draft: true
---

When it comes to storing or exchanging medium-sized sets of data, there are quite some data formats to choose from. In this article, we will compare some possible choices, namely:

* [Google Protobuf](https://developers.google.com/protocol-buffers)
* [Apache Avro](https://avro.apache.org/)
* [Apache Parquet](https://parquet.apache.org/)
* [SQLite](https://www.sqlite.org/index.html)

The source code for this comparison can be found [on GitHub](https://github.com/qaware/binary-data-format-comparison).

## Technical differences of the data-formats

### Protobuf
Protobuf serializes entities as messages into a binary representation. There is no standard way to write a large set of messages into a file, but for some languages like Java, there are convenience methods to read and write messages delimited by their size. Compression can be done with data stream compression manually.

### Avro
Avro is similar to Protobuf, but also has build-in support for writing multiple entities into a file and handles separation itself.
It has several compression codecs built-in which can be set per file.
Java classes can be generated by Avro based on an Avro schema file and are natively supported by the Avro reader and writer implementations.
Alternatively, data can be stored in [GenericRecord](https://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html).
Both variants are included in the results below.

### Parquet
Parquet is a columnar data format. It is mainly used inside the Hadoop universe, but is available as a separate library. It still depends on hadoop-common, so the dependency footprint is significantly larger than for all other formats.
It supports the schema definition files of Avro, natively supports storing multiple entities and has built-in compression which works on a per-column level, making compression very efficient.
The generated Java classes from Avro can be used as well as the GenericRecord. Additionally, Parquet allows to do the (de-)serialization without Avro. The [Group](https://javadoc.io/doc/org.apache.parquet/parquet-column/1.10.1/org/apache/parquet/example/data/Group.html) class demonstrates this. It is included in the results as well.

### SQLite
SQLite is a well-known self-contained SQL database which is supported by many languages, including Java. It can basically be used by any database framework using JDBC. The whole database with all tables is stored in a single file.
For performance comparison, a database with a single table is used and all data is retrieved with a single select.


## Performance Measurement
The data format performance was analyzed by generating a random set of data (10.000.000 entries), writing it with each format into a file and reading it back into the heap afterwards.
This was additionally done with gzip compression for each format.
The generated data contains a fixed amount of random strings (10.000) to make the effect of compression visible and more realistic.

The following metrics were taken for each storage solution:
* write time: Time to write the whole data set into a file
* read time: Time to read the whole data set from a file into the heap
* file size: memory usage on disk
* heap after GC: heap usage after loading the whole data set into the heap (excluding intermediate data)

## Results
The file size per format is as follows:

{{< figure figcaption="Diagram of file size" >}}
{{< img src="/images/binary-data-format-comparison/file-size.png" alt="Diagram of file size" >}}
{{< /figure >}}

Parquet is significantly smaller than the others. This is, because parquet will store duplicate strings much more efficient by de-duplicating the data. The additional gzip compression thus does not reduce the file size much further. The effect is much less significant when there is not so much duplication.

{{< figure figcaption="Diagram of write time" >}}
{{< img src="/images/binary-data-format-comparison/write-time.png" alt="Diagram of write time" >}}
{{< /figure >}}

Looking at the write times, Avro is by far the fastest when not compressing data, but compression takes a lot of extra time for both Avro and Protobuf.
Compressing the SQLite DB even takes several minutes and is thus not shown in the diagram to keep it readable.

{{< figure figcaption="Diagram of read time" >}}
{{< img src="/images/binary-data-format-comparison/read-time.png" alt="Diagram of read time" >}}
{{< /figure >}}

When reading the data back, Parquet is leading again, as long as data is directly mapped to the Avro-generated classes.
The example group provided by Parquet is very inefficient, both in reading performance and in heap usage.
The SQLite DB is slower than the others, as was expected, but it is a full database after all and thus provides other advantages, so it is worse considering it, if the performance overhead is acceptable.

{{< figure figcaption="Diagram of heap usage" >}}
{{< img src="/images/binary-data-format-comparison/heap.png" alt="Diagram of heap usage" >}}
{{< /figure >}}

The heap usage obviously depends on the entity class in the first place. The example group stores data redundantly and thus is twice as large as necessary.
The generic representation of Avro requires a bit more memory and the generated classes for Avro and Protobuf are similar.
The surprise is the data usage of Parquet. Due to the fact that it stores duplicate strings explicitly, it also loads each string only once into the heap.
The other formats always allocate new strings. This is a nice effect of the Parquet format, but could be mitigated by custom string pools or using `String#intern()`. 

The full set of result metrics is shown in the following two tables:

| format       | write time [s] | file size [MB] |
|--------------|----------------|----------------|
| parquet      |         15.832 |    276.4046516 |
| parquet.gzip |         20.191 |     275.317874 |
| avro         |          3.662 |    741.1535063 |
| avro.gzip    |         21.466 |    650.8264256 |
| proto        |         29.689 |    960.6373882 |
| proto.gzip   |         53.235 |    740.5153246 |
| sqlite       |         30.952 |     830.546875 |
| sqlite.gzip  |        463.966 |    669.6497068 |

| format             | class          | read time [s] | heap usage [MB]    |
|--------------------|----------------|---------------|--------------------|
| group.parquet      | SimpleGroup    |        15.614 |           9,980.34 |
| group.parquet.gzip | SimpleGroup    |         14.62 |           9,980.35 |
| gen.parquet        | SampleDataAvro |         6.526 |             597.24 |
| gen.parquet.gzip   | SampleDataAvro |         6.574 |             597.24 |
| avro               | Record         |         8.529 |           4,469.58 |
| avro.gzip          | Record         |        11.725 |           4,469.56 |
| gen.avro           | SampleDataAvro |         8.328 |           3,041.01 |
| gen.avro.gzip      | SampleDataAvro |        12.831 |           3,040.98 |
| proto              | SampleDataPb   |        10.168 |           3,193.58 |
| proto.gzip         | SampleDataPb   |         18.65 |           3,193.58 |
| sqlite             | SampleDataAvro |        27.376 |           3,133.14 |


## Summary

The analysis showed some interesting metrics of the different formats.
Parquet is a very efficient data format, but comes with a lot of dependencies and thus extra complexity. It is also less common and available tools are sparse.
Avro and Protobuf are very similar, but Avro has more features in terms of writing multiple rows and overall showed better performance when working with larger data sets.
SQLite, given it is a full SQL database, compares pretty well against the other optimized binary data formats. If you consider reading only parts of the data, consider using such a database to get more flexibility.

Note that all the metrics depend a lot on the actual data, so if you want to find the best data format for your use case, clone the GitHub project and run the benchmark with your own data format.
